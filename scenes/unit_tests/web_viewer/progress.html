<!DOCTYPE html>
<meta charset="utf-8">

**Zack's Oct 29th Update**

Introduction
============

For now please ignore all of these sections and their current ordering. These are
all of the topics that I am hoping to make progress on before next week's meeting

Bernoulli trial formulation as a special case of the pseries-cumulative estimator
=================================================================================

In this section I will first show the derivation of the Bernoulli trial estimator
, which has been used for unbiased photon gathering and unbiased specular manifold
sampling, by starting from Booth's theory. Afterwards, I will show how this same
estimator can be derived using our version of the Taylor series approach and that
it is a special case of the pseries-cumulative estimator where the majorant is set
to $1$ and the function in the denominator is a step function.

deriving the Bernoulli trial estimator from Booth's formulation
---------------------------------------------------------------

This derivation is the same derivation described within the unbiased photon
gathering paper. But before we dive into that I am going to start from Booth's
application of control variates to the reciprocal.

\begin{equation}
I = \frac{1}{\int f(x) dx}
\end{equation}

We can apply control variates by introducing an analytic control function, $g(x)$,

\begin{equation}
I = \frac{1}{\int f(x) - g(x) + g(x) dx}
\end{equation}

\begin{equation}
I = \frac{1}{\int g(x) dx - \int g(x) - f(x)dx}
\end{equation}

Let's define $G = \int g(x) dx$,

\begin{equation}
I = \frac{1}{G - \int g(x) - f(x)dx}
\end{equation}

\begin{equation}
I = \frac{\frac{1}{G}}{1 - \frac{\int g(x) - f(x) dx}{G}dx}
\end{equation}

This can also be written as,

\begin{equation}
I = \frac{\frac{1}{G}}{1 - \frac{G - \int f(x) dx}{G}dx}
\end{equation}

Booth uses the well known series expansion, $\frac{1}{1-x} - \sum_{i=0}^{\infty} x^i$,
which effectively applies the McLauren series expansion to arrive at,

\begin{equation}
I = \sum_{i=0}^{\infty} \frac{1}{G}\left(\frac{G - \int f(x) dx}{G}\right)^i
\end{equation}

In the photon gathering paper, the authors set the control variate $g(x)=1$. For
simplicity of these equations, I am going to assume that the bounds of integration
of $g(x)$ and $f(x)$ is unity. Thus, $G=1$. If not, all of the math would still
work out, but the value of $G$ (the expansion point in our formulation) would instead
be the integral of $1$ over the domain (in the photon gathering case that's over
a hemisphere),

\begin{equation}
I = \sum_{i=0}^{\infty} \frac{1}{1}\left(\frac{1 - \int f(x) dx}{1}\right)^i,
\end{equation}

\begin{equation}
I = \sum_{i=0}^{\infty} \left(1 - \int f(x) dx\right)^i.
\end{equation}

To estimate this series, it requires that we take independent samples of the
integral for each $i$, which I will denote $\hat{f}_i$,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{\infty} \prod_{j=0}^{i} 1 - \hat{f}_j.
\end{equation}

For the special case of $f(x)$ being a step function which returns either $0$ or $1$,
it does not matter what we sample after the first "success", $f(x)=1$, since due to
there now being a zero term in the running product, all future terms in the sum will
be $0$.

So to estimate the running sum, the unbiased photon gathering paper recommended to
use a continuation probability of $c(i) = 1 - \hat{f}_i$, thus, if $N$ represents
the iteration of the first success, the estimator ends up returning,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N-1} 1,
\end{equation}

due to the continuation probability always canceling out with the $ith$ contribution
and the $Nth$ term having a contribution of $0$.

I will now show that this is just a special case of the pseries-cumulative
estimator for the reciprocal. To keep the explanation rooted in the original
theory, I have written all prior derivations using the notation from both Booth
and the unbiased photon gathering paper, however, from here on out I will be
using the prior notation which I have used during every past meeting to describe
the pseries-cumulative estimator. The connections between the notations should
be obvious. The pseries-cumulative estimator is defined as,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N} \frac{1}{\tau} \prod_{k=0}^{i} \frac{\tau - x_k}{\tau p(x_k)\text{rr}(i)},
\end{equation}

where $\frac{x_k}{p(x_k)}$ is an unbiased estimate for the integral over $f(x)$,

\begin{equation}
\text{rr}(i) = \text{min}\left(1, \frac{\tau - x_i}{\tau p(x_k)} \prod_{k=0}^{i-1}\frac{\tau - x_k}{ \tau p(x_k)\text{rr}(k)} \right),
\end{equation}

is the probability of continuing. Let's now plug in the same values used in the
Bernoulli trial formulation. First let's assume that $p(x_k) = 1$,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N} \frac{1}{\tau} \prod_{k=0}^{i} \frac{\tau - x_k}{\tau\text{rr}(i)}.
\end{equation}

Now let's set the expansion point, $\tau = 1$,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N} \frac{1}{1} \prod_{k=0}^{i} \frac{1 - x_k}{1\text{rr}(i)}.
\end{equation}

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N} \prod_{k=0}^{i} \frac{1 - x_k}{\text{rr}(i)}.
\end{equation}

The russian roulette continuation probabilities then becomes,

\begin{equation}
\text{rr}(i) = \text{min}\left(1, \left(1 - x_i\right) \prod_{k=0}^{i-1}\frac{1 - x_k}{\text{rr}(k)} \right),
\end{equation}

Since $\prod_{k=0}^{i} 1 - x_k < 1$ for the case of taking the reciprocal of a
step function, the continuation probabilities should always cancel out perfectly
with the contribution in the numerator. When the first "success" occurs ($x_i = 1$),
we have,

\begin{equation}
\text{rr}(i) = \text{min}\left(1, \left(1 - 1\right) \prod_{k=0}^{i-1}\frac{1 - x_k}{\text{rr}(k)} \right),
\end{equation}

\begin{equation}
\text{rr}(i) = \text{min}\left(1, 0 \right),
\end{equation}

which will always terminate. Thus, the contribution of the pseries-cumulative
estimator for the reciprocal is,

\begin{equation}
\langle I \rangle = \sum_{i=0}^{N-1} 1,
\end{equation}

the same as the Bernoulli trial formulation when the same expansion point and
Russian Roulette is applied. The Bernoulli trial estimator is just a special
case of the pseries-cumulative estimator.

results
-------

Unfortunately, this idea of generalizing the pseries-cumulative estimator does
not work well. It seems the optimal choice of the majorant is to always set it to
1 and allow the series to degenerate into a counting process.

If we set the function in the denominator to a step function, i.e.

<img src="images/bern_cume/high_func.png"/>

then utilize both the pseries-cumulative estimator and bernoulli trial estimator
to estimate it we end up getting the following empirical results,

Cost

<img src="images/bern_cume/high_exp_cost.png"/>

Variance

<img src="images/bern_cume/high_exp_var.png"/>

Inverse efficiency

<img src="images/bern_cume/high_exp_eff.png"/>

As you can see the optimal choice of a majorant is to use the same technique as
Bernoulli trials.

constructing an Nvidia reciprocal estimator
===========================================

Constructing an analogous Nvidia style estimator for estimating reciprocals is
not as straightforward as some of the other estimators. Nvidia's estimator for
exponential transmittance has quite a few moving parts which work extremely well for
the exponential function, however, they do not generalize exactly for other functions.
In this section I will be going over a few different interpretations for the Nvidia
estimator and comparing their overall efficiency.

what's in an Nvidia estimator?
------------------------------

Here is a shorthand list of all the operations necessary to devise Nvidia's estimator
which we will need to adapt to fit the reciprocal:

Tasks:
- a taylor series reformulation with an injected parameter $c$ to control cost in an expected way.
- able to compute the cost of pseries-cmf
- a function to compute the integral samples to take per series iteration
- a way of precomputing Russian Roulette probabilities from the first bullet point
- elementary means computation
- a way of using arbitrarily sampled expansion points to combine different estimates

I will now go through item by item and discuss how these can be implemented for the
reciprocal case.

task 1
------

Nvidia's original estimator took inspiration from the Bhanot and Kennedy estimator for the
exponential,

\begin{equation}
e^x = \left( \sum_{j=0}^{K} \frac{x^j}{j!} \right) + \frac{c}{K+1}\left ( \frac{x^{K+1}}{cK!} + \frac{c}{K+2}\left (\frac{x^{K+2}}{c^2K!} + ... \right) \right).
\end{equation}

This estimator introduces a control parameter, $c$, which allows the user to control
how costly the estimator is, by choosing to utilize Russian roulette continuation
probabilities of,

\begin{equation}
p_c(i) = \frac{c}{i}
\end{equation}

Doing so results in garunteed evaluation of the first $K$ terms of the
infinite series. The pseries-cmf estimator computes $K$ such that it covers $99$
percent of the cmf and sets $c=\tau$, the majorant. We can derive the expected
cost of the Bhanot and Kennedy estimator as,

\begin{equation}
C_{BaK} = K + \left( \frac{K!}{c^K} \right)\left(e^c - \sum_{j=0}^{K}\frac{c^j}{j!} \right).
\end{equation}

The rest of Nvidia's formulations heavily relies on being able to formulate the
Taylor series so that we can apply Russian roulette termination independent
of any single evaluation of the integral. This allows for applying some additional
variance reduction techniques later on.

I will now attempt to formulate a Bhanot and Kennedy style formulation for the
Taylor series expansion of the reciprocal.

\begin{equation}
\frac{1}{x} = \sum_{i=0}^{\infty}\frac{1}{\tau}\left(\frac{\tau - x}{\tau} \right)^i
\end{equation}

\begin{equation}
\frac{1}{x} = \left(\sum_{i=0}^{K}\frac{1}{\tau}\left(\frac{\tau - x}{\tau} \right)^i \right) +
              \sum_{i=K}^{\infty}\frac{1}{\tau}\left(\frac{\tau - x}{\tau} \right)^i
\end{equation}

\begin{equation}
\frac{1}{x} = \left(\sum_{i=0}^{K}\frac{1}{\tau}\left(\frac{\tau - x}{\tau} \right)^i \right) +
              \frac{1}{\tau}\left(\frac{1}{\tau}\frac{(\tau - x)^{i+1}}{\tau^i} + \frac{1}{\tau} \left(\frac{1}{\tau}\frac{(\tau - x)^{i+2}}{\tau^i} + ... \right) \right)
\end{equation}

To make this look nicer, I am going to take out the majorant-reciprocal and bring
it to the front,

\begin{equation}
\frac{1}{x} = \frac{1}{\tau}\left[\left(\sum_{i=0}^{K}\left(\frac{\tau - x}{\tau} \right)^i \right) +
              \frac{1}{\tau}\left(\frac{(\tau - x)^{i+1}}{\tau^i} +  \frac{1}{\tau}\left(\frac{(\tau - x)^{i+2}}{\tau^i} + ... \right) \right) \right]
\end{equation}

Now let us incorporate the user-defined parameter $c$,

\begin{equation}
\frac{1}{x} = \frac{1}{\tau}\left[\left(\sum_{i=0}^{K}\left(\frac{\tau - x}{\tau} \right)^i \right) +
              \frac{c}{\tau}\left(\frac{(\tau - x)^{i+1}}{c\tau^i} +  \frac{c}{\tau}\left(\frac{(\tau - x)^{i+2}}{c^2\tau^i} + ... \right) \right) \right]
\end{equation}

At this point, there are two large discrepencies between this formulation, and the
one originally derived for the exponential. First, this formulation requires that you take the series expansion with respect to a
non-zero expansion point where the exponential case utilizes the Maclauren series.
This turns into a non-issue, however, as applying control variates to the exponential
formulation beforehand will result in something similar, and Nvidia does just that.

The second, more major discrepency, is that there is a limit to the values which
$c$ can take such that a valid estimator can be formulated here. For a valid
continuation probability to be utilized, it is obvious to see that $c < \tau$
must hold.

Finding an optimal $c$ is a task which might be something to look into in the future,
but for now, we are going to naively use $c=\frac{\tau}{2}$ unless stated otherwise.
The expected cost of this estimator is,

\begin{equation}
C_{BaKR} = K + \frac{\tau}{c}
\end{equation}

task 2
------

The Nvidia estimator is devised based on setting its cost equal to the pseries-cmf
estimator which is a conservative estimate on the contribution of the taylor expansion.
For the case of exponential transmittance, the cost of the pseries-cmf estimator
can not be solved for exactly. Depending on how you formulate the pseries-cmf estimator
for reciprocals, we can compute its exact cost.

The pseries-cmf estimator evaluates a portion, $p$ (typically 99%), of the cmf before it
starts applying Russian roulette. Unlike the Poisson distribution, the geometric
series has an analytic cmf, which results in the pseries-cmf estimator always having
an initial cost of,

\begin{equation}
\left\lceil\frac{\text{ln}\left( 1 - p\right)}{\text{ln}\left(1 - \frac{\tau_+ - \tau_-}{\tau_+} \right)}\right\rceil
\end{equation}

where $\tau_+$ and $\tau_-$ are the upper and lower bounds (majorant and minorant),
respectively. Afterwards, the pseries-cmf estimator can either use a Russian roulette
continuation probability of $\frac{1}{\tau_+}$ to terminate, or choose to continue
based off of the cumulative weight. The cumulative weight termination criteria
will terminate nearly immediately, while the former will terminate after an expected
$\tau_+$ iterations. The former performs better, but the latter is more conservative
and has an analytic formulation.

Therefore, we have two choices for how we can decide what cost we want to match:

\begin{equation}
C\left[ \langle P_{\text{cmf}} \rangle \right]_1 = \left\lceil\frac{\text{ln}\left( 1 - p\right)}{\text{ln}\left(1 - \frac{\tau_+ - \tau_-}{\tau_+} \right)}\right\rceil + \tau_+
\end{equation}

or my naive approximation of the former,

\begin{equation}
C\left[ \langle P_{\text{cmf}} \rangle \right]_2 = \left\lceil\frac{\text{ln}\left( 1 - p\right)}{\text{ln}\left(1 - \frac{\tau_+ - \tau_-}{\tau_+} \right)}\right\rceil + 1
\end{equation}

task 3
------

Nvidia chooses to set the overall cost of their estimator equal to pseries-cmf.
Based off of the expected cost of task 1 and the expected cost of task 2, they
compute $M$, the number of integral samples to take every Taylor series iteration
as,

\begin{equation}
M = \text{max}\left(\frac{C\left[ \langle P_{\text{cmf}} \rangle \right]}{C\left[ \text{BandK}_R \right] + 1} + 0.5 \right)
\end{equation}

This step is no different than the transmittance paper.

task 4
------

In the transmittance paper, only the first term is evaluated $90$ percent of the
time. For the remaining $10$ percent, the number of extra Taylor series iterations
which are evaluated is determined by using Russian Roulette continuation probabilities
of $\frac{c}{i}$ where $i$ is the series iteration. For the reciprocal case, this
process is the same except the Russian Roulette continuation probabilities are
$\frac{c}{\tau}$.

task 5
------

Elementary means computation is no different then the implementation in Nvidia's
paper.

task 6
------

The Nvidia estimator makes one additional evaluation ($N+1$ total) of the integral
so that it can combine $N$ estimates by using all different $N$ permutations of
the samples such that the order of the $N$ evaluations stay the same. i.e. The first
integral evaluation will either be included first, or not at all. The second evaluation
will either be included first, second, or not at all. The third evaluation will
either be included second, third, or not at all, etc. The integral evaluation which
was chosen to be not included is then used as a pivot.

This works out extremely well for exponential transmittance where the optimal pivot
is approximately the expected value, however, this is not the case for the reciprocal.
Estimating the taylor series expansion of the reciprocal can diverge if a bad pivot
is chosen, which is guaranteed to happen if the pivot is chosen by randomly evaluating
the integral. This means that this variance reduction technique is not possible to
use exactly as it was used in the transmittance paper. We can still combine $N$
different evaluations of the reciprocal, however, we must always use the same, known
majorant as the pivot.

This requirement might be relaxed if more information is known regarding the
specific problem this estimator is being applied to, but I am not sure at the moment
how.

reciprocal estimation comparisons
=================================

offline, hopefully

specular-manifold-sampling debiasing
====================================

Compare irl offline (sorry ran out of time to convert exrs to pngs)

<!--- Markdeep & image comparison library - probably no need to change anything below -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="resources/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="resources/jquery.event.move.js"></script>
<script src="resources/jquery.twentytwenty.js"></script>
<!-- <script src="jeri.js"></script> -->
<link href="resources/offcanvas.css" rel="stylesheet">
<link href="resources/twentytwenty.css" rel="stylesheet" type="text/css" />
<script>
$(window).load(function(){$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5});});
</script>
